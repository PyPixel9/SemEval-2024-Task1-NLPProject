{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "DvKNoSA2sLJJ",
        "_GAcHnF5xINd",
        "mPbKl67CxLd6",
        "fAl2zH1iDui_"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 - ERC (Emotion Recognition in conversation)"
      ],
      "metadata": {
        "id": "ymlyJmFruh_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup"
      ],
      "metadata": {
        "id": "6pe-dbZ0sGy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install transformers\n",
        "# ! pip install sentence_transformers"
      ],
      "metadata": {
        "id": "LofP6I-Ldiyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "zx7jBe7Xdjf_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ik1PbluhgX4s",
        "outputId": "c4e725e0-8695-4408-bfd7-8f27eb8a4ec1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "0eSJjNx6dpBH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data *engneering*"
      ],
      "metadata": {
        "id": "DvKNoSA2sLJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# folder_path = \"/content/drive/MyDrive/IIITD/Courses/nlp/Assignment 4/\"\n",
        "# path_train_file = folder_path+\"train_file.json\"\n",
        "# df_train = pd.read_json(path_train_file)\n",
        "# df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "tDnChSGwdr2t",
        "outputId": "11bf08c3-ae4d-4e87-c23e-0c3937fb8ee1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          episode                                           speakers  \\\n",
              "0  utterance_3492               [Phoebe, Eric, Phoebe, Eric, Phoebe]   \n",
              "1  utterance_3952  [Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...   \n",
              "2  utterance_3198  [Older Scientist, Ross, Ross, Joey, Ross, Ross...   \n",
              "3  utterance_2834                           [Monica, Monica, Monica]   \n",
              "4   utterance_453                         [Kate, The Director, Kate]   \n",
              "\n",
              "                                            emotions  \\\n",
              "0       [surprise, fear, surprise, sadness, disgust]   \n",
              "1  [disgust, disgust, anger, sadness, surprise, a...   \n",
              "2  [neutral, neutral, neutral, neutral, neutral, ...   \n",
              "3                       [neutral, surprise, neutral]   \n",
              "4                            [joy, sadness, sadness]   \n",
              "\n",
              "                                          utterances  \\\n",
              "0  [You-you\n",
              "you had sex with Ursula?!, Uh, a litt...   \n",
              "1  [Dad, please don't pick your teeth out here!, ...   \n",
              "2  [Dr. Geller, there's a seat over here., Thank ...   \n",
              "3  [So, how'd the lasagne go over?, Really?!, Good.]   \n",
              "4  [Become a drama critic!, I am hurt!  A plague ...   \n",
              "\n",
              "                                        triggers  \n",
              "0                                [1, 1, 0, 0, 0]  \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
              "2                          [0, 0, 0, 0, 1, 1, 1]  \n",
              "3                                      [0, 0, 1]  \n",
              "4                                      [0, 0, 1]  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bed6a227-a591-4773-be6f-6d2809801bd8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode</th>\n",
              "      <th>speakers</th>\n",
              "      <th>emotions</th>\n",
              "      <th>utterances</th>\n",
              "      <th>triggers</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>utterance_3492</td>\n",
              "      <td>[Phoebe, Eric, Phoebe, Eric, Phoebe]</td>\n",
              "      <td>[surprise, fear, surprise, sadness, disgust]</td>\n",
              "      <td>[You-youyou had sex with Ursula?!, Uh, a litt...</td>\n",
              "      <td>[1, 1, 0, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>utterance_3952</td>\n",
              "      <td>[Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...</td>\n",
              "      <td>[disgust, disgust, anger, sadness, surprise, a...</td>\n",
              "      <td>[Dad, please don't pick your teeth out here!, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>utterance_3198</td>\n",
              "      <td>[Older Scientist, Ross, Ross, Joey, Ross, Ross...</td>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n",
              "      <td>[Dr. Geller, there's a seat over here., Thank ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>utterance_2834</td>\n",
              "      <td>[Monica, Monica, Monica]</td>\n",
              "      <td>[neutral, surprise, neutral]</td>\n",
              "      <td>[So, how'd the lasagne go over?, Really?!, Good.]</td>\n",
              "      <td>[0, 0, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>utterance_453</td>\n",
              "      <td>[Kate, The Director, Kate]</td>\n",
              "      <td>[joy, sadness, sadness]</td>\n",
              "      <td>[Become a drama critic!, I am hurt!  A plague ...</td>\n",
              "      <td>[0, 0, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bed6a227-a591-4773-be6f-6d2809801bd8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bed6a227-a591-4773-be6f-6d2809801bd8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bed6a227-a591-4773-be6f-6d2809801bd8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1fdae0d9-137c-4ba8-88c9-9452a68f935c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1fdae0d9-137c-4ba8-88c9-9452a68f935c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1fdae0d9-137c-4ba8-88c9-9452a68f935c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 6740,\n  \"fields\": [\n    {\n      \"column\": \"episode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3854,\n        \"samples\": [\n          \"utterance_2812\",\n          \"utterance_1212\",\n          \"utterance_2157\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speakers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"utterances\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"triggers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in df_train['utterances']:\n",
        "#     print(i)\n",
        "#     break ;"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaOKyLtOdunr",
        "outputId": "3ef6b9f6-daff-4c0c-b126-b8a7e9f25066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['You-you\\x85you had sex with Ursula?!', 'Uh, a little bit. She-she-she walked in and I thought she was you and I kissed her and', \"You didn't notice she was wearing different clothes?!\", 'Well I was just so excited to see you.', \"Oh. Ew! Ew! Ew! Ugh! Y'know what? This is too weird.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = SentenceTransformer('all-mpnet-base-v2')"
      ],
      "metadata": {
        "id": "donvcMPadwgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train['utterances_embeddings'] = np.nan\n",
        "\n",
        "# df_train.head()"
      ],
      "metadata": {
        "id": "he-PENU-dyzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if os.path.exists('sentence_embeddings.pkl'):\n",
        "#     embeddings = pickle.load(open('sentence_embeddings.pkl', 'rb'))\n",
        "# else :\n",
        "#     embeddings = [model.encode(utterance) for utterance in tqdm(df_train['utterances'])]\n",
        "#     pickle.dump(embeddings, open('sentence_embeddings.pkl', 'wb'))\n",
        "\n",
        "# df_train['utterances_embeddings'] = embeddings\n",
        "\n",
        "# df_train.head()"
      ],
      "metadata": {
        "id": "cyC2AREer7Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train.info()"
      ],
      "metadata": {
        "id": "NRSP1-9GuNBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def generate_cnn_embeddings(row):\n",
        "#     speakers = row['speakers']\n",
        "\n",
        "#     embeddings_list = row['utterances_embeddings']\n",
        "\n",
        "#     speaker_index = {}\n",
        "#     index = 0\n",
        "#     for speaker in speakers:\n",
        "#         if speaker not in speaker_index:\n",
        "#             speaker_index[speaker] = index\n",
        "#             index += 1\n",
        "\n",
        "#     S = len(speaker_index)\n",
        "#     N = len(speakers)\n",
        "#     embeddings_tensor = np.zeros((max(10 , S), 768, N))\n",
        "#     for i, (speaker, embedding) in enumerate(zip(speakers, embeddings_list)):\n",
        "#         idx = speaker_index[speaker]\n",
        "#         embeddings_tensor[idx, :, i] = embedding\n",
        "#     return embeddings_tensor\n",
        "\n",
        "\n",
        "# df_train['CNN_embeddings'] = df_train.apply(generate_cnn_embeddings, axis=1)"
      ],
      "metadata": {
        "id": "gNdH_ItOd92S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pickle.dump(df_train, open('df_train_with_CNN_embeddings.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "Okn6uOnZeAOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_train = pickle.load(open('df_train_with_CNN_embeddings.pkl', 'rb'))\n",
        "\n",
        "# df_train.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "7I0qoxKBeDrZ",
        "outputId": "c256f12e-d86c-44fc-e573-81bfe0122714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          episode                                           speakers  \\\n",
              "0  utterance_3492               [Phoebe, Eric, Phoebe, Eric, Phoebe]   \n",
              "1  utterance_3952  [Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...   \n",
              "2  utterance_3198  [Older Scientist, Ross, Ross, Joey, Ross, Ross...   \n",
              "3  utterance_2834                           [Monica, Monica, Monica]   \n",
              "4   utterance_453                         [Kate, The Director, Kate]   \n",
              "\n",
              "                                            emotions  \\\n",
              "0       [surprise, fear, surprise, sadness, disgust]   \n",
              "1  [disgust, disgust, anger, sadness, surprise, a...   \n",
              "2  [neutral, neutral, neutral, neutral, neutral, ...   \n",
              "3                       [neutral, surprise, neutral]   \n",
              "4                            [joy, sadness, sadness]   \n",
              "\n",
              "                                          utterances  \\\n",
              "0  [You-you\n",
              "you had sex with Ursula?!, Uh, a litt...   \n",
              "1  [Dad, please don't pick your teeth out here!, ...   \n",
              "2  [Dr. Geller, there's a seat over here., Thank ...   \n",
              "3  [So, how'd the lasagne go over?, Really?!, Good.]   \n",
              "4  [Become a drama critic!, I am hurt!  A plague ...   \n",
              "\n",
              "                                        triggers  \\\n",
              "0                                [1, 1, 0, 0, 0]   \n",
              "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]   \n",
              "2                          [0, 0, 0, 0, 1, 1, 1]   \n",
              "3                                      [0, 0, 1]   \n",
              "4                                      [0, 0, 1]   \n",
              "\n",
              "                               utterances_embeddings  \\\n",
              "0  [[0.020053696, 0.048883013, -0.037799813, -0.0...   \n",
              "1  [[0.0045765517, 0.025685206, -0.02977001, 0.02...   \n",
              "2  [[0.063676834, 0.084198244, 0.0095247915, -0.0...   \n",
              "3  [[-0.00056250574, 0.07043089, -0.0339791, -0.0...   \n",
              "4  [[0.043867074, 0.019203804, -0.011715968, -0.0...   \n",
              "\n",
              "                                      CNN_embeddings  \n",
              "0  [[[0.020053695887327194, 0.0, -0.0219856929033...  \n",
              "1  [[[0.004576551727950573, 0.0251162052154541, 0...  \n",
              "2  [[[0.06367683410644531, 0.0, 0.0, 0.0, 0.0, 0....  \n",
              "3  [[[-0.0005625057383440435, 0.02774389833211898...  \n",
              "4  [[[0.0438670739531517, 0.0, 0.0092357126995921...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5d15ae4-ae18-4b69-ac3e-95124de5d13a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>episode</th>\n",
              "      <th>speakers</th>\n",
              "      <th>emotions</th>\n",
              "      <th>utterances</th>\n",
              "      <th>triggers</th>\n",
              "      <th>utterances_embeddings</th>\n",
              "      <th>CNN_embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>utterance_3492</td>\n",
              "      <td>[Phoebe, Eric, Phoebe, Eric, Phoebe]</td>\n",
              "      <td>[surprise, fear, surprise, sadness, disgust]</td>\n",
              "      <td>[You-youyou had sex with Ursula?!, Uh, a litt...</td>\n",
              "      <td>[1, 1, 0, 0, 0]</td>\n",
              "      <td>[[0.020053696, 0.048883013, -0.037799813, -0.0...</td>\n",
              "      <td>[[[0.020053695887327194, 0.0, -0.0219856929033...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>utterance_3952</td>\n",
              "      <td>[Monica, Monica, Phoebe, Joey, Joey, Joey, Rac...</td>\n",
              "      <td>[disgust, disgust, anger, sadness, surprise, a...</td>\n",
              "      <td>[Dad, please don't pick your teeth out here!, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
              "      <td>[[0.0045765517, 0.025685206, -0.02977001, 0.02...</td>\n",
              "      <td>[[[0.004576551727950573, 0.0251162052154541, 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>utterance_3198</td>\n",
              "      <td>[Older Scientist, Ross, Ross, Joey, Ross, Ross...</td>\n",
              "      <td>[neutral, neutral, neutral, neutral, neutral, ...</td>\n",
              "      <td>[Dr. Geller, there's a seat over here., Thank ...</td>\n",
              "      <td>[0, 0, 0, 0, 1, 1, 1]</td>\n",
              "      <td>[[0.063676834, 0.084198244, 0.0095247915, -0.0...</td>\n",
              "      <td>[[[0.06367683410644531, 0.0, 0.0, 0.0, 0.0, 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>utterance_2834</td>\n",
              "      <td>[Monica, Monica, Monica]</td>\n",
              "      <td>[neutral, surprise, neutral]</td>\n",
              "      <td>[So, how'd the lasagne go over?, Really?!, Good.]</td>\n",
              "      <td>[0, 0, 1]</td>\n",
              "      <td>[[-0.00056250574, 0.07043089, -0.0339791, -0.0...</td>\n",
              "      <td>[[[-0.0005625057383440435, 0.02774389833211898...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>utterance_453</td>\n",
              "      <td>[Kate, The Director, Kate]</td>\n",
              "      <td>[joy, sadness, sadness]</td>\n",
              "      <td>[Become a drama critic!, I am hurt!  A plague ...</td>\n",
              "      <td>[0, 0, 1]</td>\n",
              "      <td>[[0.043867074, 0.019203804, -0.011715968, -0.0...</td>\n",
              "      <td>[[[0.0438670739531517, 0.0, 0.0092357126995921...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5d15ae4-ae18-4b69-ac3e-95124de5d13a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c5d15ae4-ae18-4b69-ac3e-95124de5d13a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c5d15ae4-ae18-4b69-ac3e-95124de5d13a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8fd68a87-d482-4d79-bb3e-ddae16d1d7fe\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8fd68a87-d482-4d79-bb3e-ddae16d1d7fe')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8fd68a87-d482-4d79-bb3e-ddae16d1d7fe button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_train",
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 6740,\n  \"fields\": [\n    {\n      \"column\": \"episode\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3854,\n        \"samples\": [\n          \"utterance_2812\",\n          \"utterance_1212\",\n          \"utterance_2157\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"speakers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotions\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"utterances\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"triggers\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"utterances_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CNN_embeddings\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Models**"
      ],
      "metadata": {
        "id": "DZGuyBGWd3DT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DataLoader"
      ],
      "metadata": {
        "id": "11cpCpITt8rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, json_path = 'train_file.json', spacial_speaker_embedding=False):\n",
        "        self.embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
        "        self.spacial_speaker_embedding=spacial_speaker_embedding\n",
        "        self.train_data = pd.read_json(json_path)\n",
        "        emotions = ['surprise', 'neutral', 'disgust', 'fear', 'sadness', 'anger', 'joy']\n",
        "        self.emotion_dict = {}\n",
        "        for i, emotion in enumerate(emotions):\n",
        "            self.emotion_dict[emotion] = i\n",
        "\n",
        "    def generate_cnn_embeddings(self, row):\n",
        "        speakers = row['speakers']\n",
        "\n",
        "        embeddings_list = self.embedding_model.encode(row['utterances'])\n",
        "\n",
        "        speaker_index = {}\n",
        "        index = 0\n",
        "        for speaker in speakers:\n",
        "            if speaker not in speaker_index:\n",
        "                speaker_index[speaker] = index\n",
        "                index += 1\n",
        "\n",
        "        S = len(speaker_index)\n",
        "        N = len(speakers)\n",
        "        embeddings_tensor = np.zeros((max(10 , S), N, 768))\n",
        "        for i, (speaker, embedding) in enumerate(zip(speakers, embeddings_list)):\n",
        "            idx = speaker_index[speaker]\n",
        "            embeddings_tensor[idx, i, :] = embedding\n",
        "        return embeddings_tensor\n",
        "\n",
        "    def generate_embeddings(self,row):\n",
        "        embeddings_list = self.embedding_model.encode(row['utterances'])\n",
        "        return embeddings_list\n",
        "\n",
        "    def gen_emotion_matrix(self, row):\n",
        "        emotions = row['emotions']\n",
        "        emotion_matrix = np.zeros((len(emotions), 7))\n",
        "        for idx, emotion in enumerate(emotions):\n",
        "            emotion_matrix[idx][self.emotion_dict[emotion]] = 1\n",
        "        return emotion_matrix\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.train_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.spacial_speaker_embedding:\n",
        "            X = self.generate_cnn_embeddings(self.train_data.iloc[idx])\n",
        "        else :\n",
        "            X = self.generate_embeddings(self.train_data.iloc[idx])\n",
        "        Y = self.gen_emotion_matrix(self.train_data.iloc[idx])\n",
        "        return X, Y"
      ],
      "metadata": {
        "id": "IsxcMmfTuAh7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "tKESJpdcsdNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ERC_Attention(nn.Module):\n",
        "    def __init__(self, num_heads=8, input_size=768, hidden_size=64, num_layers=1, mlp_hidden_size=64, output_size=7):\n",
        "        super(ERC_Attention, self).__init__()\n",
        "        \"\"\"\n",
        "        EERC_Attention Module combining Attention, BiLSTM, and MLP\n",
        "        Input shape: (batch_size, in_channels, width, length)\n",
        "        Output shape: (batch_size, seq_length, output_size)\n",
        "        \"\"\"\n",
        "        # self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # Attention layers\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(input_size, num_heads=num_heads)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # MLP layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, mlp_hidden_size)\n",
        "        self.fc2 = nn.Linear(mlp_hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Attention\n",
        "        utterance_embeddings = x\n",
        "        x, _ = self.multihead_attn(utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
        "\n",
        "        # BiLSTM\n",
        "        x = x.squeeze(1)  # Remove the singleton dimension\n",
        "        x, _ = self.bilstm(x)  # out shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # MLP\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = self.softmax(x) # softmax is not letting it train fast atleast\n",
        "\n",
        "        return x\n",
        "\n",
        "class ERC_CNN(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels, input_size, hidden_size, num_layers, mlp_hidden_size, output_size):\n",
        "        super(ERC_CNN, self).__init__()\n",
        "        \"\"\"\n",
        "        ERC-CNN Module combining CNN, BiLSTM, and MLP\n",
        "        Input shape: (batch_size, in_channels, width, length)\n",
        "        Output shape: (batch_size, seq_length, output_size)\n",
        "        \"\"\"\n",
        "        # self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        # self.num_layers = num_layers\n",
        "        # CNN layers\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=(3, 3), padding=1)\n",
        "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=(3, 3), padding=1)\n",
        "\n",
        "        # BiLSTM layer\n",
        "        # self.h0, self.c0 = torch.zeros(self.num_layers * 2, self.input_size, self.hidden_size), torch.zeros(self.num_layers * 2, self.input_size, self.hidden_size)\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "        # MLP layers\n",
        "        self.fc1 = nn.Linear(hidden_size * 2, mlp_hidden_size)\n",
        "        self.fc2 = nn.Linear(mlp_hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim = 1)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "\n",
        "        # BiLSTM\n",
        "        x = x.squeeze(1)  # Remove the singleton dimension\n",
        "        x, _ = self.bilstm(x)  # out shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # MLP\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        # x = self.softmax(x) # softmax is not letting it train fast atleast\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "x2u6fAXLBJHF"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define parameters\n",
        "batch_size = 1\n",
        "speakers = 10  # max_speakers\n",
        "num_utterances = 24  # dialogue_length\n",
        "embedding_size = 768\n",
        "\n",
        "cnn_mid_channels = 3\n",
        "cnn_out_channels = 1\n",
        "\n",
        "hidden_lstm = 64\n",
        "layers_lstm = 4\n",
        "\n",
        "inputs_mlp = hidden_lstm * 2\n",
        "hidden_mlp = 64\n",
        "output_mlp = number_of_emotions = 7\n",
        "\n",
        "\n",
        "# Create individual components\n",
        "# Initialize the ERC_CNN class\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ERC_CNN(in_channels=speakers,\n",
        "                mid_channels=cnn_mid_channels,\n",
        "                out_channels=cnn_out_channels,\n",
        "                input_size=embedding_size,\n",
        "                hidden_size=hidden_lstm,\n",
        "                num_layers=layers_lstm,\n",
        "                mlp_hidden_size=hidden_mlp,\n",
        "                output_size=output_mlp).to(DEVICE)\n",
        "\n",
        "num_heads = 2\n",
        "model_2 = ERC_Attention(num_heads=num_heads,\n",
        "                input_size=embedding_size,\n",
        "                hidden_size=hidden_lstm,\n",
        "                num_layers=layers_lstm,\n",
        "                mlp_hidden_size=hidden_mlp,\n",
        "                output_size=output_mlp).to(DEVICE)\n",
        "\n",
        "# input_tensor = torch.randn(batch_size, num_utterances, embedding_size)\n",
        "# # output = model(input_tensor)\n",
        "# output = model_2(input_tensor)\n",
        "# print(\"Final Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "s6ZRgvvkS9bj"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your model, loss function, and optimizer\n",
        "folder_path = \"/content/drive/MyDrive/IIITD/Courses/nlp/Assignment 4/\"\n",
        "# folder_path = \"\"\n",
        "path_train_file = folder_path+\"train_file.json\"\n",
        "path_val_file = folder_path+\"val_file.json\"\n",
        "\n",
        "spacial_speaker_embedding = True\n",
        "train_dataset = MyDataset(path_train_file,spacial_speaker_embedding)\n",
        "val_dataset = MyDataset(path_val_file,spacial_speaker_embedding)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = model.to(DEVICE)\n",
        "torch.save(model.state_dict(), 'cnn.pth')\n",
        "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "from tqdm import tqdm\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.load_state_dict(torch.load('cnn.pth'))\n",
        "    running_loss = 0.0\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in tqdm(enumerate(train_dataloader)):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        labels = labels.to(DEVICE)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs.float())\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    # Print average loss for the epoch\n",
        "    epoch_loss = running_loss / len(train_dataloader.dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    torch.save(model.state_dict(), 'cnn.pth')\n",
        "\n",
        "    if(epoch%3==2):\n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        for i, (inputs, labels) in tqdm(enumerate(val_dataloader)):\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            labels = labels.to(DEVICE)\n",
        "\n",
        "            # optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "            outputs = model(inputs.float())\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Print average loss for the epoch\n",
        "        val_epoch_loss = val_running_loss / len(val_dataloader.dataset)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Val Loss: {val_epoch_loss:.4f}\")"
      ],
      "metadata": {
        "id": "xZfwGkcOrwnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "_GAcHnF5xINd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#! add speaker embedding maybe self attention on sparse\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(embedding_size, num_heads)\n",
        "\n",
        "    def forward(self, utterance_embeddings):\n",
        "        # Reshape utterance embeddings to (seq_len, batch_size, embedding_dim)\n",
        "        # utterance_embeddings = utterance_embeddings.unsqueeze(0)  # Add a dimension for seq_len\n",
        "        output, _ = self.multihead_attn(utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "# num_utterances = 24  # Number of utterances\n",
        "# # Generate random utterance embeddings\n",
        "# utterance_embeddings = torch.randn(1, num_utterances, embedding_size)\n",
        "\n",
        "s = 10  # max_speakers\n",
        "n = 24  # dialogue_length\n",
        "num_heads = 8  # Number of attention heads\n",
        "embedding_size = 768  # Size of each embedding\n",
        "input = torch.randn(s, n, embedding_size)\n",
        "\n",
        "\n",
        "# Define self-attention layer\n",
        "attention_layer = SelfAttention(embedding_size=embedding_size, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(input)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)  # Output shape: (n, embedding_dim)"
      ],
      "metadata": {
        "id": "GvGu7fIKxKjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "091b2970-fddd-4b8c-b2b7-4fe1fe071e72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 24, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bi-LSTM"
      ],
      "metadata": {
        "id": "mPbKl67CxLd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # 2 for bidirectional\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM : (h_n, c_n): A tuple containing the final hidden state $h_n$ and the final cell state $c_n$ of the LSTM,\n",
        "        out, _ = self.bilstm(x, (h0, c0))  # out shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # Concatenate the outputs from both directions\n",
        "        out = torch.cat((out[:, :, :self.hidden_size], out[:, :, self.hidden_size:]), dim=2)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define input size, hidden size, and number of layers\n",
        "input_size = 768\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "\n",
        "n = 24\n",
        "\n",
        "# Create BiLSTM instance\n",
        "bilstm = BiLSTM(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Generate random input tensor\n",
        "input_tensor = torch.randn(1, n, 768)  # Batch size 24, input size 768\n",
        "input_tensor = output\n",
        "\n",
        "# Forward pass through BiLSTM\n",
        "output = bilstm(input_tensor)  # Add batch dimension\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Output shape should be [1, 24, 128] (24 vectors each of size 128)"
      ],
      "metadata": {
        "id": "mjhgm4nin3Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP + Softmax"
      ],
      "metadata": {
        "id": "fAl2zH1iDui_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "# class EmotionMLP(nn.Module):\n",
        "#     def __init__(self, input_size, hidden_size, output_size):\n",
        "#         super(EmotionMLP, self).__init__()\n",
        "#         self.relu = nn.ReLU()\n",
        "#         self.fc_1 = nn.Linear(input_size, hidden_size)\n",
        "#         self.fc_2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.fc_1(x)\n",
        "#         x = self.relu(x)\n",
        "#         x = self.fc_2(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# # Define input size, hidden size, and output size for the MLP\n",
        "# input_size = 128\n",
        "# hidden_size = 64\n",
        "# output_size = 6\n",
        "\n",
        "# n = 24\n",
        "\n",
        "# # Create a shared MLP instance\n",
        "# shared_mlp = EmotionMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# # Generate random input tensor\n",
        "# input_tensor = torch.randn(1, n, 128)  # Number of utterances: n = 24, Utterance embedding size: 128\n",
        "# input_tensor = output\n",
        "\n",
        "# # Apply the shared MLP sequentially 24 times\n",
        "# outputs = []\n",
        "# for i in range(n):\n",
        "#     output = shared_mlp(input_tensor[:, i, :])  # Feed each 128-dimensional vector\n",
        "#     output = F.softmax(output, dim=1)  # Apply softmax along dimension 1\n",
        "#     outputs.append(output.unsqueeze(1))  # Add a singleton dimension for concatenation later\n",
        "\n",
        "# # Concatenate the outputs along the second dimension to get 24 6-dimensional outputs\n",
        "# final_output = torch.cat(outputs, dim=1).squeeze(0)\n",
        "\n",
        "# print(\"Final output shape:\", final_output)  # Output shape should be [24, 6]"
      ],
      "metadata": {
        "id": "F684YxMKrznG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2xPbR-3byNp"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# s = 10  # max_speakers\n",
        "# n = 24  # dialogue_length\n",
        "# input = torch.randn(s, n, 768)\n",
        "\n",
        "# # Define the parameters for the convolution operation\n",
        "# mid_channels = 3\n",
        "# out_channels = 1  # Number of output channels\n",
        "# kernel_size = (1, 1, 1)  # Kernel size (depth, height, width)\n",
        "# stride = 1\n",
        "\n",
        "# # Define the convolutional layer\n",
        "# conv3d_1 = nn.Conv3d(in_channels=s, out_channels=mid_channels, kernel_size=kernel_size)\n",
        "# conv3d_2 = nn.Conv3d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "\n",
        "# # Reshape input to (batch_size, in_channels, depth, height, width)\n",
        "# input_ = input.unsqueeze(0).unsqueeze(2)  # Adding batch and depth dimensions\n",
        "\n",
        "# # Perform convolution\n",
        "# mid = conv3d_1(input_)\n",
        "# output = conv3d_2(mid)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(input_.shape)\n",
        "# print(mid.shape)\n",
        "# print(output.shape)\n",
        "\n",
        "# # Ensure the output has the desired shape\n",
        "# output = output.squeeze(0).squeeze(1)  # Remove batch and depth dimensions\n",
        "# print(\"Output shape:\", output.shape)  # Output shape should be 1*n*768"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "U7uRQxKrxAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, input_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(input_size, num_heads)\n",
        "\n",
        "    def forward(self, sentence_embeddings):\n",
        "        output, _ = self.multihead_attn(sentence_embeddings, sentence_embeddings, sentence_embeddings)\n",
        "        return output\n",
        "\n",
        "sentence_embeddings = torch.randn(20, 10, 768)  # Shape: (seq_len, batch_size, embedding_dim)\n",
        "\n",
        "# Define self-attention layer\n",
        "num_heads = 1  # Number of attention heads\n",
        "attention_layer = SelfAttention(input_size=768, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(sentence_embeddings)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)"
      ],
      "metadata": {
        "id": "GO211Hdip-AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(embedding_size, num_heads)\n",
        "\n",
        "    def forward(self, utterance_embeddings):\n",
        "        # Reshape utterance embeddings to (seq_len, batch_size, embedding_dim)\n",
        "        utterance_embeddings = utterance_embeddings.unsqueeze(0)  # Add a dimension for seq_len\n",
        "        output, _ = self.multihead_attn(utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
        "        return output.squeeze(0)  # Remove the added dimension\n",
        "\n",
        "# Example usage\n",
        "num_utterances = 10  # Number of utterances\n",
        "embedding_size = 768  # Size of each embedding\n",
        "num_heads = 8  # Number of attention heads\n",
        "\n",
        "# Generate random utterance embeddings\n",
        "utterance_embeddings = torch.randn(num_utterances, embedding_size)\n",
        "\n",
        "# Define self-attention layer\n",
        "attention_layer = SelfAttention(embedding_size=embedding_size, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(utterance_embeddings)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)  # Output shape: (n, embedding_dim)"
      ],
      "metadata": {
        "id": "aQnkjZewq7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bbbddbb-aefb-45e7-e273-7c8677ec41ae"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels):\n",
        "        super(CNN, self).__init__()\n",
        "        \"\"\"\n",
        "        CNN Module\n",
        "        Input shape: (batch_size, in_channels, width, length)\n",
        "        Output shape: (batch_size, out_channels, width, length)\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=(1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = f.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        BiLSTM Module\n",
        "        Input shape: (batch_size, seq_length, input_size)\n",
        "        Output shape: (batch_size, seq_length, hidden_size * 2)\n",
        "        \"\"\"\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.bilstm(x)\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        \"\"\"\n",
        "        MLP Module\n",
        "        Input shape: (batch_size, seq_length, input_size)\n",
        "        Output shape: (batch_size, seq_length, output_size)\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ERC_CNN(nn.Module):\n",
        "    # torch.Size([16, 10, 24, 768])\n",
        "    # torch.Size([16, 1, 24, 768])\n",
        "    # torch.Size([16, 24, 768])\n",
        "    # torch.Size([16, 24, 128])\n",
        "    # torch.Size([16, 24, 128])\n",
        "    # torch.Size([16, 24, 6])\n",
        "\n",
        "    # Final Output shape: torch.Size([16, 24, 6])\n",
        "\n",
        "    def __init__(self, cnn, bilstm, mlp):\n",
        "        super(ERC_CNN, self).__init__()\n",
        "        self.cnn = cnn\n",
        "        self.bilstm = bilstm\n",
        "        self.mlp = mlp\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        x = self.cnn(x)  # Output shape: (batch_size, out_channels, width, length)\n",
        "\n",
        "        # BiLSTM\n",
        "        x = x.squeeze(1)  # Remove the singleton dimension\n",
        "        x = self.bilstm(x)  # Output shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # MLP\n",
        "        x = self.mlp(x)  # Output shape: (batch_size, seq_length, output_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 1\n",
        "speakers = 10  # max_speakers\n",
        "num_utterances = 24  # dialogue_length\n",
        "embedding_size = 768\n",
        "\n",
        "cnn_mid_channels = 3\n",
        "cnn_out_channels = 1\n",
        "\n",
        "hidden_lstm = 64\n",
        "layers_lstm = 1\n",
        "\n",
        "inputs_mlp = hidden_lstm * 2\n",
        "hidden_mlp = 64\n",
        "output_mlp = number_of_emotions = 7\n",
        "\n",
        "\n",
        "# Create individual components\n",
        "cnn = CNN(speakers, cnn_mid_channels, cnn_out_channels)\n",
        "bilstm = BiLSTM(embedding_size, hidden_lstm, layers_lstm)\n",
        "mlp = MLP(inputs_mlp, hidden_mlp, output_mlp)\n",
        "\n",
        "# # Create the combined model\n",
        "# model = ERC_CNN(cnn, bilstm, mlp)\n",
        "# input_tensor = torch.randn(batch_size, speakers, num_utterances, embedding_size)\n",
        "\n",
        "\n",
        "# # Forward pass through the model\n",
        "# output = model(input_tensor)  # Add batch dimension\n",
        "\n",
        "# print(\"Final Output shape:\", output.shape)"
      ],
      "metadata": {
        "id": "ocxc_IqLtQwv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}