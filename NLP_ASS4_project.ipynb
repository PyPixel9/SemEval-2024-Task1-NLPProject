{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "smgcJu_GusqE",
        "_GAcHnF5xINd",
        "mPbKl67CxLd6",
        "fAl2zH1iDui_",
        "U7uRQxKrxAEZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Model 1 - ERC (Emotion Recognition in conversation)"
      ],
      "metadata": {
        "id": "ymlyJmFruh_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# torch.Size([16, 10, 24, 768])\n",
        "# torch.Size([16, 1, 24, 768])\n",
        "# torch.Size([16, 24, 768])\n",
        "# torch.Size([16, 24, 128])\n",
        "# torch.Size([16, 24, 128])\n",
        "# torch.Size([16, 24, 6])\n",
        "\n",
        "# Final Output shape: torch.Size([16, 24, 6])\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_channels, mid_channels, out_channels):\n",
        "        super(CNN, self).__init__()\n",
        "        \"\"\"\n",
        "        CNN Module\n",
        "        Input shape: (batch_size, in_channels, width, length)\n",
        "        Output shape: (batch_size, out_channels, width, length)\n",
        "        \"\"\"\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=(1, 1))\n",
        "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=(1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        \"\"\"\n",
        "        BiLSTM Module\n",
        "        Input shape: (batch_size, seq_length, input_size)\n",
        "        Output shape: (batch_size, seq_length, hidden_size * 2)\n",
        "        \"\"\"\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.bilstm(x)\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(MLP, self).__init__()\n",
        "        \"\"\"\n",
        "        MLP Module\n",
        "        Input shape: (batch_size, seq_length, input_size)\n",
        "        Output shape: (batch_size, seq_length, output_size)\n",
        "        \"\"\"\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class ERC_CNN(nn.Module):\n",
        "    def __init__(self, cnn, bilstm, mlp):\n",
        "        super(ERC_CNN, self).__init__()\n",
        "        self.cnn = cnn\n",
        "        self.bilstm = bilstm\n",
        "        self.mlp = mlp\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN\n",
        "        x = self.cnn(x)  # Output shape: (batch_size, out_channels, width, length)\n",
        "\n",
        "        # BiLSTM\n",
        "        x = x.squeeze(1)  # Remove the singleton dimension\n",
        "        x = self.bilstm(x)  # Output shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # MLP\n",
        "        x = self.mlp(x)  # Output shape: (batch_size, seq_length, output_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define parameters\n",
        "batch_size = 16\n",
        "speakers = 10  # max_speakers\n",
        "num_utterances = 24  # dialogue_length\n",
        "embedding_size = 768\n",
        "\n",
        "cnn_mid_channels = 3\n",
        "cnn_out_channels = 1\n",
        "\n",
        "hidden_lstm = 64\n",
        "layers_lstm = 1\n",
        "\n",
        "inputs_mlp = hidden_lstm * 2\n",
        "hidden_mlp = 64\n",
        "output_mlp = number_of_emotions = 6\n",
        "\n",
        "\n",
        "# Create individual components\n",
        "cnn = CNN(speakers, cnn_mid_channels, cnn_out_channels)\n",
        "bilstm = BiLSTM(embedding_size, hidden_lstm, layers_lstm)\n",
        "mlp = MLP(inputs_mlp, hidden_mlp, output_mlp)\n",
        "\n",
        "# Create the combined model\n",
        "model = ERC_CNN(cnn, bilstm, mlp)\n",
        "\n",
        "# Generate random input tensor\n",
        "input_tensor = torch.randn(batch_size, speakers, num_utterances, embedding_size)\n",
        "\n",
        "# Forward pass through the model\n",
        "output = model(input_tensor)  # Add batch dimension\n",
        "\n",
        "print(\"Final Output shape:\", output.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2u6fAXLBJHF",
        "outputId": "7f21e2fb-fac5-4aaa-a298-b63c198411fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final Output shape: torch.Size([16, 24, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN"
      ],
      "metadata": {
        "id": "smgcJu_GusqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "s = 10  # max_speakers\n",
        "n = 24  # dialogue_length\n",
        "input = torch.randn(s, n, 768)\n",
        "\n",
        "# Define the parameters for the convolution operation\n",
        "mid_channels = 3\n",
        "out_channels = 1  # Number of output channels\n",
        "kernel_size = (1, 1)  # Kernel size (height, width)\n",
        "stride = 1\n",
        "\n",
        "# Define the convolutional layer\n",
        "conv3d_1 = nn.Conv2d(in_channels=s, out_channels=mid_channels, kernel_size=kernel_size)\n",
        "conv3d_2 = nn.Conv2d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "# Perform convolution\n",
        "mid = conv3d_1(input)\n",
        "output = conv3d_2(mid)\n",
        "\n",
        "print(input.shape)\n",
        "print(mid.shape)\n",
        "print(output.shape)\n",
        "\n",
        "# Ensure the output has the desired shape\n",
        "print(\"Output shape:\", output.shape)  # Output shape should be 1*n*768"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CbfGLxjjNeL",
        "outputId": "adca7496-0dd1-4255-9923-42049be02e89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 24, 768])\n",
            "torch.Size([3, 24, 768])\n",
            "torch.Size([1, 24, 768])\n",
            "Output shape: torch.Size([1, 24, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "_GAcHnF5xINd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "#! add speaker embedding maybe self attention on sparse\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(embedding_size, num_heads)\n",
        "\n",
        "    def forward(self, utterance_embeddings):\n",
        "        # Reshape utterance embeddings to (seq_len, batch_size, embedding_dim)\n",
        "        # utterance_embeddings = utterance_embeddings.unsqueeze(0)  # Add a dimension for seq_len\n",
        "        output, _ = self.multihead_attn(utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
        "        return output\n",
        "\n",
        "# Example usage\n",
        "# num_utterances = 24  # Number of utterances\n",
        "# # Generate random utterance embeddings\n",
        "# utterance_embeddings = torch.randn(1, num_utterances, embedding_size)\n",
        "\n",
        "s = 10  # max_speakers\n",
        "n = 24  # dialogue_length\n",
        "num_heads = 8  # Number of attention heads\n",
        "embedding_size = 768  # Size of each embedding\n",
        "input = torch.randn(s, n, embedding_size)\n",
        "\n",
        "\n",
        "# Define self-attention layer\n",
        "attention_layer = SelfAttention(embedding_size=embedding_size, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(input)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)  # Output shape: (n, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GvGu7fIKxKjZ",
        "outputId": "b2017812-c5c4-43f2-df9c-665943568098"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 24, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bi-LSTM"
      ],
      "metadata": {
        "id": "mPbKl67CxLd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Bidirectional LSTM layer\n",
        "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden state and cell state\n",
        "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)  # 2 for bidirectional\n",
        "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
        "\n",
        "        # Forward propagate LSTM : (h_n, c_n): A tuple containing the final hidden state $h_n$ and the final cell state $c_n$ of the LSTM,\n",
        "        out, _ = self.bilstm(x, (h0, c0))  # out shape: (batch_size, seq_length, hidden_size * 2)\n",
        "\n",
        "        # Concatenate the outputs from both directions\n",
        "        out = torch.cat((out[:, :, :self.hidden_size], out[:, :, self.hidden_size:]), dim=2)\n",
        "\n",
        "        return out\n",
        "\n",
        "# Define input size, hidden size, and number of layers\n",
        "input_size = 768\n",
        "hidden_size = 64\n",
        "num_layers = 1\n",
        "\n",
        "n = 24\n",
        "\n",
        "# Create BiLSTM instance\n",
        "bilstm = BiLSTM(input_size, hidden_size, num_layers)\n",
        "\n",
        "# Generate random input tensor\n",
        "input_tensor = torch.randn(1, n, 768)  # Batch size 24, input size 768\n",
        "input_tensor = output\n",
        "\n",
        "# Forward pass through BiLSTM\n",
        "output = bilstm(input_tensor)  # Add batch dimension\n",
        "\n",
        "print(\"Output shape:\", output.shape)  # Output shape should be [1, 24, 128] (24 vectors each of size 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mjhgm4nin3Qa",
        "outputId": "fdef9692-ad0e-4c06-9239-c4ae2c5b4729"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 24, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP + Softmax"
      ],
      "metadata": {
        "id": "fAl2zH1iDui_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class EmotionMLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(EmotionMLP, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc_1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc_2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc_2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Define input size, hidden size, and output size for the MLP\n",
        "input_size = 128\n",
        "hidden_size = 64\n",
        "output_size = 6\n",
        "\n",
        "n = 24\n",
        "\n",
        "# Create a shared MLP instance\n",
        "shared_mlp = EmotionMLP(input_size, hidden_size, output_size)\n",
        "\n",
        "# Generate random input tensor\n",
        "input_tensor = torch.randn(1, n, 128)  # Number of utterances: n = 24, Utterance embedding size: 128\n",
        "input_tensor = output\n",
        "\n",
        "# Apply the shared MLP sequentially 24 times\n",
        "outputs = []\n",
        "for i in range(n):\n",
        "    output = shared_mlp(input_tensor[:, i, :])  # Feed each 128-dimensional vector\n",
        "    output = F.softmax(output, dim=1)  # Apply softmax along dimension 1\n",
        "    outputs.append(output.unsqueeze(1))  # Add a singleton dimension for concatenation later\n",
        "\n",
        "# Concatenate the outputs along the second dimension to get 24 6-dimensional outputs\n",
        "final_output = torch.cat(outputs, dim=1).squeeze(0)\n",
        "\n",
        "print(\"Final output shape:\", final_output)  # Output shape should be [24, 6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F684YxMKrznG",
        "outputId": "3fb03994-f525-4fd5-c4a5-a40a0dd08b81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final output shape: tensor([[0.1715, 0.1687, 0.1636, 0.1813, 0.1558, 0.1591],\n",
            "        [0.1742, 0.1655, 0.1609, 0.1858, 0.1506, 0.1629],\n",
            "        [0.1669, 0.1639, 0.1596, 0.1933, 0.1518, 0.1645],\n",
            "        [0.1666, 0.1675, 0.1603, 0.1886, 0.1520, 0.1649],\n",
            "        [0.1680, 0.1675, 0.1578, 0.1851, 0.1542, 0.1673],\n",
            "        [0.1658, 0.1724, 0.1589, 0.1861, 0.1524, 0.1644],\n",
            "        [0.1625, 0.1651, 0.1624, 0.1926, 0.1533, 0.1641],\n",
            "        [0.1688, 0.1619, 0.1610, 0.1936, 0.1523, 0.1623],\n",
            "        [0.1638, 0.1631, 0.1607, 0.1951, 0.1531, 0.1642],\n",
            "        [0.1660, 0.1607, 0.1563, 0.1992, 0.1557, 0.1621],\n",
            "        [0.1657, 0.1638, 0.1566, 0.1942, 0.1537, 0.1660],\n",
            "        [0.1719, 0.1619, 0.1600, 0.1913, 0.1499, 0.1650],\n",
            "        [0.1709, 0.1608, 0.1612, 0.1933, 0.1481, 0.1656],\n",
            "        [0.1703, 0.1632, 0.1563, 0.1931, 0.1525, 0.1646],\n",
            "        [0.1698, 0.1602, 0.1571, 0.1909, 0.1551, 0.1669],\n",
            "        [0.1723, 0.1632, 0.1571, 0.1896, 0.1539, 0.1639],\n",
            "        [0.1723, 0.1621, 0.1610, 0.1868, 0.1519, 0.1658],\n",
            "        [0.1726, 0.1632, 0.1627, 0.1896, 0.1538, 0.1581],\n",
            "        [0.1674, 0.1646, 0.1591, 0.1910, 0.1552, 0.1627],\n",
            "        [0.1710, 0.1645, 0.1603, 0.1896, 0.1503, 0.1642],\n",
            "        [0.1744, 0.1645, 0.1574, 0.1842, 0.1502, 0.1694],\n",
            "        [0.1703, 0.1655, 0.1567, 0.1886, 0.1490, 0.1699],\n",
            "        [0.1702, 0.1715, 0.1599, 0.1818, 0.1508, 0.1658],\n",
            "        [0.1706, 0.1671, 0.1591, 0.1834, 0.1518, 0.1680]],\n",
            "       grad_fn=<SqueezeBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2xPbR-3byNp",
        "outputId": "0b05fd3d-337d-44c1-e22f-15861b9cbe3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 24, 768])\n",
            "torch.Size([1, 10, 1, 24, 768])\n",
            "torch.Size([1, 3, 1, 24, 768])\n",
            "torch.Size([1, 1, 1, 24, 768])\n",
            "Output shape: torch.Size([1, 24, 768])\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# s = 10  # max_speakers\n",
        "# n = 24  # dialogue_length\n",
        "# input = torch.randn(s, n, 768)\n",
        "\n",
        "# # Define the parameters for the convolution operation\n",
        "# mid_channels = 3\n",
        "# out_channels = 1  # Number of output channels\n",
        "# kernel_size = (1, 1, 1)  # Kernel size (depth, height, width)\n",
        "# stride = 1\n",
        "\n",
        "# # Define the convolutional layer\n",
        "# conv3d_1 = nn.Conv3d(in_channels=s, out_channels=mid_channels, kernel_size=kernel_size)\n",
        "# conv3d_2 = nn.Conv3d(in_channels=mid_channels, out_channels=out_channels, kernel_size=kernel_size)\n",
        "\n",
        "\n",
        "# # Reshape input to (batch_size, in_channels, depth, height, width)\n",
        "# input_ = input.unsqueeze(0).unsqueeze(2)  # Adding batch and depth dimensions\n",
        "\n",
        "# # Perform convolution\n",
        "# mid = conv3d_1(input_)\n",
        "# output = conv3d_2(mid)\n",
        "\n",
        "# print(input.shape)\n",
        "# print(input_.shape)\n",
        "# print(mid.shape)\n",
        "# print(output.shape)\n",
        "\n",
        "# # Ensure the output has the desired shape\n",
        "# output = output.squeeze(0).squeeze(1)  # Remove batch and depth dimensions\n",
        "# print(\"Output shape:\", output.shape)  # Output shape should be 1*n*768"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention"
      ],
      "metadata": {
        "id": "U7uRQxKrxAEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, input_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(input_size, num_heads)\n",
        "\n",
        "    def forward(self, sentence_embeddings):\n",
        "        output, _ = self.multihead_attn(sentence_embeddings, sentence_embeddings, sentence_embeddings)\n",
        "        return output\n",
        "\n",
        "sentence_embeddings = torch.randn(20, 10, 768)  # Shape: (seq_len, batch_size, embedding_dim)\n",
        "\n",
        "# Define self-attention layer\n",
        "num_heads = 1  # Number of attention heads\n",
        "attention_layer = SelfAttention(input_size=768, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(sentence_embeddings)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO211Hdip-AA",
        "outputId": "cb0b8bbf-7d64-452a-ae91-e3383bc8a20d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embedding_size, num_heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead_attn = nn.MultiheadAttention(embedding_size, num_heads)\n",
        "\n",
        "    def forward(self, utterance_embeddings):\n",
        "        # Reshape utterance embeddings to (seq_len, batch_size, embedding_dim)\n",
        "        utterance_embeddings = utterance_embeddings.unsqueeze(0)  # Add a dimension for seq_len\n",
        "        output, _ = self.multihead_attn(utterance_embeddings, utterance_embeddings, utterance_embeddings)\n",
        "        return output.squeeze(0)  # Remove the added dimension\n",
        "\n",
        "# Example usage\n",
        "num_utterances = 10  # Number of utterances\n",
        "embedding_size = 768  # Size of each embedding\n",
        "num_heads = 8  # Number of attention heads\n",
        "\n",
        "# Generate random utterance embeddings\n",
        "utterance_embeddings = torch.randn(num_utterances, embedding_size)\n",
        "\n",
        "# Define self-attention layer\n",
        "attention_layer = SelfAttention(embedding_size=embedding_size, num_heads=num_heads)\n",
        "\n",
        "# Apply self-attention\n",
        "context_embeddings = attention_layer(utterance_embeddings)\n",
        "\n",
        "# Check the shape of the output tensor\n",
        "print(context_embeddings.shape)  # Output shape: (n, embedding_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQnkjZewq7da",
        "outputId": "fa9c787a-ee77-40fd-c9f3-02e67d2dee6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([10, 768])\n"
          ]
        }
      ]
    }
  ]
}